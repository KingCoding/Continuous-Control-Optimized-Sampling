{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a907780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/student/.local/lib/python3.11/site-packages (25.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef2dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d6c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/student/.local/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/student/.local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/student/.local/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/student/.local/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/student/.local/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/student/.local/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/student/.local/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/student/.local/lib/python3.11/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/student/.local/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/student/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/student/.local/lib/python3.11/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/student/.local/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student/.local/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.1 #protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.protobuf\n",
    "print(google.protobuf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e7cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75577bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /data/Reacher_Linux_NoVis/Reacher.x86_64\n",
      "Mono path[0] = '/data/Reacher_Linux_NoVis/Reacher_Data/Managed'\n",
      "Mono config path = '/data/Reacher_Linux_NoVis/Reacher_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "Logging to /home/student/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name='Reacher_Windows_x86_64/Reacher.exe', base_port=63457)\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879b8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6165351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d882d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents) # initialize the score (for each agent)\n",
    "i = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    #print(actions)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "#     print(states[0])\n",
    "#     print(actions[0:2])\n",
    "#     print(actions2[0:2])\n",
    "    i += 1\n",
    "    if i >= 4 :                                        # Exit after 4 iterations.\n",
    "         break\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48cc8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Main code taken from Udacity's repository:\n",
    "##https://github.com/udacity/deep-reinforcement-learning/\n",
    "##\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##Hidden layer's weights intialization\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        _input_layer = 128\n",
    "        _hidden_1 = 256\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, _input_layer)\n",
    "        self.fc2 = nn.Linear(_input_layer, _hidden_1)\n",
    "        self.fc3 = nn.Linear(_hidden_1, action_size)\n",
    "        \n",
    "        #batchnorm\n",
    "        self.bn_input = nn.BatchNorm1d(_input_layer)\n",
    "        self.bn_hidden = nn.BatchNorm1d(_hidden_1)\n",
    "\n",
    "        #Dropout\n",
    "        self.dpout = nn.Dropout(p=0.20)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        #If there are multiple windows, the state will have them already concatenated\n",
    "        #Input layer: Dense + Batchnorm + ReLU\n",
    "        x = self.fc1(state) \n",
    "        x = self.bn_input(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #First hidden layer: Dense + ReLU\n",
    "        x = self.fc2(x)\n",
    "#         x = self.bn_hidden(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #Output layer: Dense  + Tanh\n",
    "        x = self.fc3(x)\n",
    "        out = torch.tanh(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        _input_layer = 128\n",
    "        _hidden_1 = 256\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fcs1 = nn.Linear(state_size, _input_layer)\n",
    "        self.fc2 = nn.Linear(_input_layer + action_size, _hidden_1) #concat the action from the actor\n",
    "        self.fc3 = nn.Linear(_hidden_1, 1)\n",
    "        \n",
    "        #batchnorm\n",
    "        self.bn_input = nn.BatchNorm1d(_input_layer)\n",
    "        \n",
    "        #Dropout\n",
    "        self.dpout = nn.Dropout(p=0.20)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        #If there are multiple windows, the state will have them already concatenated\n",
    "        #Input layer: Dense + Batchnorm + ReLU\n",
    "        x = self.fcs1(state) \n",
    "        x = self.bn_input(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #First hidden layer Dense + ReLU\n",
    "        #Concat state & action\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #Output layer: Dense\n",
    "        out = self.fc3(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35691b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "MIN_LATEST_PER_BATCH = 1 # 0 also works well but 1 worked better for these hyperparameters\n",
    "MAX_LATEST_PER_BATCH = 4#3#2#5#4#5#6#5#4#3 # max number of latest experiences to add in each trainig batch\n",
    "\n",
    "GAMMA = 0.95            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "TRAIN_EVERY = 20        # How many iterations to wait before updating target networks\n",
    "NUM_AGENTS = 20         # How many agents are there in the environment\n",
    "\n",
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, idle_steps, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.max_latest_experiences = num_agents*idle_steps\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR) #, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        #Copy the weights from local to target networks\n",
    "        self.soft_update(self.critic_local, self.critic_target, 1)\n",
    "        self.soft_update(self.actor_local, self.actor_target, 1)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise((NUM_AGENTS, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, self.max_latest_experiences, MIN_LATEST_PER_BATCH, MAX_LATEST_PER_BATCH, random_seed)\n",
    "        \n",
    "        \n",
    "    def step(self, states, actions, rewards, next_states, dones, step):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        \n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            # Save experience / reward for all the agents\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "\n",
    "        if len(self.memory) > BATCH_SIZE and (len(self.memory) % self.max_latest_experiences) == 0:\n",
    "            while self.memory.hasLatestExperiences():                \n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def act(self, state, add_noise=True):        \n",
    "\n",
    "        #Convert the state array to tensor\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "            \n",
    "        #Perform a training step\n",
    "        self.actor_local.train()\n",
    "        #Add noise to the obtained action      \n",
    "        if add_noise:\n",
    "            for act in actions:\n",
    "                act += self.noise.sample() #make sure each individual action has a unique noise added to it\n",
    "        \n",
    "        #Ensure the output is in the valid action's range [-1, 1]\n",
    "        return np.clip(actions, -1, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:            \n",
    "            actions += self.noise.sample() #make sure each individual action has a unique noise added to it\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        # Get action from actor's network, given the NEXT state        \n",
    "        actions_next = self.actor_target(next_states)\n",
    "        # Use the obtained action as input to the critic's network, along with the NEXT state\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next).detach()\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        #Get the expected value from the critic's local network\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        \n",
    "        #Gradient clipping\n",
    "        ##torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        #Get the \"best\" action using the actor's local network, given current state\n",
    "        actions_pred = self.actor_local(states)\n",
    "        \n",
    "        #Compute the loss by getting the expected value (V) from the critic's local network,\n",
    "        #given the current state and the obtained action. \n",
    "        #Set it negative to perform gradient ascent?\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        #Gradient clipping\n",
    "        ##torch.nn.utils.clip_grad_norm_(self.actor_local.parameters(), 1)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "                \n",
    "        \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, shape, seed, mu=0., theta=0.15, sigma=0.08):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(shape)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * (np.random.rand(*x.shape)-0.5)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, max_latest_experiences, min_latest_per_batch, max_latest_per_batch, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.latest_experiences: list[self.experience] = []\n",
    "        self.curr_latest_index = -1\n",
    "        self.max_latest_experiences = max_latest_experiences\n",
    "        self.max_latest_per_batch = max_latest_per_batch\n",
    "        self.min_latest_per_batch = min_latest_per_batch\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "                \n",
    "        \n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(WeightedExperience(e,0))\n",
    "        self.z1c = self.z1c + 1\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        if len(self.memory) < self.max_latest_experiences:\n",
    "            self.memory.append(e)\n",
    "        else:\n",
    "            self.latest_experiences.append(e) \n",
    "        \n",
    "        \"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        #self.memory.append(e)\n",
    "        if len(self.memory) == BUFFER_SIZE:            \n",
    "            self.first_index = (self.first_index + 1) % BUFFER_SIZE\n",
    "        \n",
    "        self.last_index = (self.last_index + 1) % BUFFER_SIZE\n",
    "\n",
    "        if len(self.memory) < BUFFER_SIZE:\n",
    "            self.memory.append(WeightedExperience(e,0))\n",
    "        else:\n",
    "            self.memory[self.last_index] = WeightedExperience(e,0)\n",
    "        \n",
    "        self.z1c = self.z1c + 1\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        latestList = None\n",
    "        \n",
    "        if len(self.latest_experiences) == 0:\n",
    "            latestList = self.memory\n",
    "        else:\n",
    "            latestList = self.latest_experiences\n",
    "        \n",
    "        \n",
    "        if self.curr_latest_index == -1:\n",
    "            random.shuffle(latestList)\n",
    "            self.curr_latest_index = 0\n",
    "        \n",
    "        #numsOflatest = [1,2,3,5]\n",
    "        numsOflatest = [1,3,4] # with 20 idle steps and 20 agents, this yields 150 trainings per iteration\n",
    "        numLatestsToAdd = min( len(latestList) - self.curr_latest_index, random.choice(numsOflatest))\n",
    "        #numLatestsToAdd = min( len(latestList) - self.curr_latest_index, random.randint(self.min_latest_per_batch, self.max_latest_per_batch))\n",
    "        \n",
    "        latestsToAdd = random.sample(range(len(experiences)), numLatestsToAdd)\n",
    "        \n",
    "        for i in latestsToAdd:\n",
    "            if latestList is self.latest_experiences:\n",
    "                experiences[i] = latestList[self.curr_latest_index]\n",
    "            #if latestList is self.latest_experiences:\n",
    "            #self.memory.append(latestList[self.curr_latest_index])\n",
    "            self.curr_latest_index = self.curr_latest_index + 1\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "        #experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    \n",
    "    def hasLatestExperiences(self):\n",
    "        if self.curr_latest_index < self.max_latest_experiences:\n",
    "            return True\n",
    "        else:\n",
    "            self.curr_latest_index = -1\n",
    "            self.memory.extend(self.latest_experiences)\n",
    "            self.latest_experiences.clear()\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory) + len(self.latest_experiences)\n",
    "    \n",
    "    def zero1Count(self):\n",
    "        return self.z1c\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        #return len(self.memory)\n",
    "        return len(self.memory) + len(self.latest_experiences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dedf637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Episode 1\tCumulative Average Score: 0.51\n",
      "Episode 2\tCumulative Average Score: 0.84\n",
      "Episode 3\tCumulative Average Score: 1.31\n",
      "Episode 4\tCumulative Average Score: 1.33\n",
      "Episode 5\tCumulative Average Score: 1.94\n",
      "Episode 6\tCumulative Average Score: 2.28\n",
      "Episode 7\tCumulative Average Score: 2.67\n",
      "Episode 8\tCumulative Average Score: 2.44\n",
      "Episode 9\tCumulative Average Score: 2.94\n",
      "Episode 10\tCumulative Average Score: 3.28\n",
      "Episode 11\tCumulative Average Score: 3.87\n",
      "Episode 12\tCumulative Average Score: 4.09\n",
      "Episode 13\tCumulative Average Score: 4.69\n",
      "Episode 14\tCumulative Average Score: 6.04\n",
      "Episode 15\tCumulative Average Score: 6.47\n",
      "Episode 16\tCumulative Average Score: 8.84\n",
      "Episode 17\tCumulative Average Score: 8.45\n",
      "Episode 18\tCumulative Average Score: 10.93\n",
      "Episode 19\tCumulative Average Score: 8.71\n",
      "Episode 20\tCumulative Average Score: 12.12\n",
      "Episode 21\tCumulative Average Score: 12.91\n",
      "Episode 22\tCumulative Average Score: 14.14\n",
      "Episode 23\tCumulative Average Score: 17.68\n",
      "Episode 24\tCumulative Average Score: 16.80\n",
      "Episode 25\tCumulative Average Score: 20.83\n",
      "Episode 26\tCumulative Average Score: 20.85\n",
      "Episode 27\tCumulative Average Score: 21.57\n",
      "Episode 28\tCumulative Average Score: 21.22\n",
      "Episode 29\tCumulative Average Score: 22.55\n",
      "Episode 30\tCumulative Average Score: 24.28\n",
      "Episode 31\tCumulative Average Score: 25.00\n",
      "Episode 32\tCumulative Average Score: 26.57\n",
      "Episode 33\tCumulative Average Score: 27.63\n",
      "Episode 34\tCumulative Average Score: 28.93\n",
      "Episode 35\tCumulative Average Score: 29.67\n",
      "Episode 36\tCumulative Average Score: 30.65\n",
      "Episode 37\tCumulative Average Score: 31.66\n",
      "Episode 38\tCumulative Average Score: 31.71\n",
      "Episode 39\tCumulative Average Score: 33.91\n",
      "Episode 40\tCumulative Average Score: 31.53\n",
      "Episode 41\tCumulative Average Score: 35.02\n",
      "Episode 42\tCumulative Average Score: 35.97\n",
      "Episode 43\tCumulative Average Score: 37.15\n",
      "Episode 44\tCumulative Average Score: 37.72\n",
      "Episode 45\tCumulative Average Score: 37.69\n",
      "Episode 46\tCumulative Average Score: 38.17\n",
      "Episode 47\tCumulative Average Score: 38.54\n",
      "Episode 48\tCumulative Average Score: 39.15\n",
      "Episode 49\tCumulative Average Score: 38.83\n",
      "Episode 50\tCumulative Average Score: 39.20\n",
      "Episode 51\tCumulative Average Score: 39.36\n",
      "Episode 52\tCumulative Average Score: 39.25\n",
      "Episode 53\tCumulative Average Score: 39.16\n",
      "Episode 54\tCumulative Average Score: 39.17\n",
      "Episode 55\tCumulative Average Score: 38.59\n",
      "Episode 56\tCumulative Average Score: 38.79\n",
      "Episode 57\tCumulative Average Score: 38.89\n",
      "Episode 58\tCumulative Average Score: 38.93\n",
      "Episode 59\tCumulative Average Score: 38.97\n",
      "Episode 60\tCumulative Average Score: 38.98\n",
      "Episode 61\tCumulative Average Score: 38.38\n",
      "Episode 62\tCumulative Average Score: 38.98\n",
      "Episode 63\tCumulative Average Score: 39.05\n",
      "Episode 64\tCumulative Average Score: 39.04\n",
      "Episode 65\tCumulative Average Score: 38.93\n",
      "Episode 66\tCumulative Average Score: 39.08\n",
      "Episode 67\tCumulative Average Score: 39.31\n",
      "Episode 68\tCumulative Average Score: 39.19\n",
      "Episode 69\tCumulative Average Score: 38.89\n",
      "Episode 70\tCumulative Average Score: 39.16\n",
      "Episode 71\tCumulative Average Score: 38.81\n",
      "Episode 72\tCumulative Average Score: 38.80\n",
      "Episode 73\tCumulative Average Score: 39.35\n",
      "Episode 74\tCumulative Average Score: 38.91\n",
      "Episode 75\tCumulative Average Score: 39.16\n",
      "Episode 76\tCumulative Average Score: 38.98\n",
      "Episode 77\tCumulative Average Score: 39.20\n",
      "Episode 78\tCumulative Average Score: 39.06\n",
      "Episode 79\tCumulative Average Score: 38.92\n",
      "Episode 80\tCumulative Average Score: 38.64\n",
      "Episode 81\tCumulative Average Score: 38.61\n",
      "Episode 82\tCumulative Average Score: 38.40\n",
      "Episode 83\tCumulative Average Score: 39.18\n",
      "Episode 84\tCumulative Average Score: 38.77\n",
      "Episode 85\tCumulative Average Score: 38.87\n",
      "Episode 86\tCumulative Average Score: 39.21\n",
      "Episode 87\tCumulative Average Score: 38.89\n",
      "Episode 88\tCumulative Average Score: 39.01\n",
      "Episode 89\tCumulative Average Score: 39.28\n",
      "Episode 90\tCumulative Average Score: 38.82\n",
      "Episode 91\tCumulative Average Score: 38.73\n",
      "Episode 92\tCumulative Average Score: 38.65\n",
      "Episode 93\tCumulative Average Score: 38.25\n",
      "Episode 94\tCumulative Average Score: 37.38\n",
      "Episode 95\tCumulative Average Score: 37.91\n",
      "Episode 96\tCumulative Average Score: 38.52\n",
      "Episode 97\tCumulative Average Score: 37.67\n",
      "Episode 98\tCumulative Average Score: 38.02\n",
      "Episode 99\tCumulative Average Score: 38.44\n",
      "Episode 100\tCumulative Average Score: 38.49\n",
      "Episode 100\tAverage Score: 29.02\n",
      "Episode 101\tCumulative Average Score: 37.98\n",
      "Episode 102\tCumulative Average Score: 37.94\n",
      "Episode 103\tCumulative Average Score: 37.81\n",
      "Environment solved in 103 episodes, mean score: 30.13\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "scores = []                                                   # A list to store the scores of all episodes\n",
    "\n",
    "# The DDPG implementation\n",
    "def ddpg(n_episodes=200, max_t=1000, print_every=100): # Normaly n_episodes=1000. But it was reduced to 300 to save GPU time\n",
    "    \n",
    "    scores_deque = deque(maxlen=print_every)                  # A queue to keep only the last 100 episodes' scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #print(\"ep:\", i_episode)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "\n",
    "        #states is an array of the states for all the 20 agents.\n",
    "        states = env_info.vector_observations                 # get the current state (for each agent)\n",
    "        score = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        \n",
    "        agent.reset()                                         # Reset the agent to start the episode\n",
    "        for t in range(max_t):                                # A loop for the iterations\n",
    "            actions = agent.act(states)#, False)                       # Get an action from the Actor's network, given the current state\n",
    "            env_info = env.step(actions)[brain_name]          # send all actions to the environment\n",
    "            next_states = env_info.vector_observations        # get next state (for each agent)\n",
    "            rewards = env_info.rewards                        # get reward (for each agent)\n",
    "            dones = env_info.local_done                       # see if episode finished\n",
    "            score += env_info.rewards                         # update the score (for each agent)\n",
    "            \n",
    "            # Perform an agent step. The step function expects one tuple (s,a,r,ns) at a time.\n",
    "            #for i in range(20) :\n",
    "            #agent.step(states[i], actions[i], rewards[i], next_states[i], dones[i], t)\n",
    "            agent.step(states, actions, rewards, next_states, dones, t)\n",
    "            states = next_states                              # Roll over states to next time step\n",
    "\n",
    "            if np.any(dones):                                 # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "        score = score.mean()                                  # Get the mean score of the episode (over all agents)\n",
    "        scores_deque.append(score)                            # Store the score in the queue\n",
    "        scores.append(score)                                  # Store the score in the list (for plotting)\n",
    "        \n",
    "        # Print out the mean score per episode\n",
    "        #print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score), end=\"\")\n",
    "        print('\\rEpisode {}\\tCumulative Average Score: {:.2f}'.format(i_episode, score))\n",
    "\n",
    "        mean_sc = np.mean(scores_deque)                       # Compute the mean score over the last 100 episodes\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_sc))\n",
    "            \n",
    "        if len(scores_deque) == 100 and mean_sc >= 30 :\n",
    "            print('\\rEnvironment solved in {} episodes, mean score: {:.2f}'.format(i_episode, mean_sc))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    " \n",
    "print(\"start\")\n",
    "#agent = Agent(state_size=state_size, action_size=action_size, random_seed=2)\n",
    "act_size = action_size\n",
    "st_size = state_size\n",
    "n_agents = 20\n",
    "id_stps = 20#15#20#10#20\n",
    "\n",
    "agent = Agent(state_size=st_size, action_size=act_size, num_agents=n_agents, idle_steps=id_stps, random_seed=2)  #correct the right size of state and action\n",
    "\n",
    "scores = ddpg()\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7994e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a3636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6785312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
